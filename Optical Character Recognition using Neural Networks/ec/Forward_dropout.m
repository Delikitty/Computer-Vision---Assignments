function [output, act_h, act_a, dropind] = Forward_dropout(W, b, X)
% [OUT, act_h, act_a] = Forward(W, b, X) performs forward propogation on the
% input data 'X' uisng the network defined by weights and biases 'W' and 'b'
% (as generated by InitializeNetwork(..)).
% This function should return the final softmax output layer activations in OUT,
% as well as the hidden layer pre activations in 'act_h', and the hidden layer post
% activations in 'act_a'.


% Inplement of Forward with dropout=0.5
len = length(W);
act_a = cell(len,1);
act_h = cell(len,1);
dropind = cell(len,1);

for i=1:len
    % random index to set half output to 0
    if i~=1
        [row,col] = size(act_h{i-1});
        randind = randperm(row*col,0.5*row*col);
        drop = act_h{i-1};
        drop(randind) = 0;
        drop = 2.* drop;
        dropind{i} = randind;
    end
    % rescale the output after drpoout
    %% The input layer
    if i==1
        act_a{i} = W{i} * X' + b{i};
        act_h{i} = 1./(1 + exp(-act_a{i}));
    end
    %% The output layer
    if i==len
        
        act_a{i} = W{i} * drop + b{i};
        output = exp(act_a{i}) ./ sum(sum(exp(act_a{i})));
        act_h{len} = output;
    end
    %% Other layers
    if i~=1 && i~=len
        act_a{i} = W{i} * drop + b{i};
        act_h{i} = 1./(1 + exp(-act_a{i}));
    end
    
        
end

end
